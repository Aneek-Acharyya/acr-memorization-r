# memorization-in-llms
A compression-based approach to defining and measuring memorization with LLMs.

This repository contains the code needed to measure memorization in LLMs using input-output compression. It was developed collaboratively by Avi Schwarzschild, Zhili Feng, and Pratyush Maini at Carnegie Mellon University in 2024. This code is particularly useful for reproducing the resutls in our paper on the topic.


## Getting Started

### Requirements

### Optimizing Prompts

```
% python example_script.py
```

### Memorization Measurements

```
% python miniprompt-main.py
```

### Logging Style and Data Analysis

```
outputs
└── happy-Melissa
        ├── .hydra
        │   ├── config.yaml
        │   ├── hydra.yaml
        │   └── overrides.yaml
        ├── results.json
        └── log.log
```

## Contributing

We encourage anyone using the code to reachout to us directly and open issues and pull requests with questions and improvements!

## Citing Our Work


